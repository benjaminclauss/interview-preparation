{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"General Tips Try to solve the problem on your own. Write the code on paper. Test your code - on paper. Type your paper code as-is into a computer. Do as many mock interviews as possible. Practicing implementing the data structures and algorithm (on paper, and then on a computer) is also a great exercise. Make sure to implement it the moment after, and a day later, and even revisit it a week later briefly, to make sure you understand the solution. mkdocs For documentation visit mkdocs.org . mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Patterns 14 Patterns to Ace Any Coding Interview Question Leetcode Pattern 0 | Iterative traversals on Trees Leetcode Pattern 1 | BFS + DFS == 25% of the problems \u2014 part 1 Leetcode Pattern 1 | DFS + BFS == 25% of the problems \u2014 part 2 Leetcode Pattern 2 | Sliding Windows for Strings Leetcode Pattern 3 | Backtracking Leetcode Pattern 4 | Meta Stuff","title":"Home"},{"location":"#general-tips","text":"Try to solve the problem on your own. Write the code on paper. Test your code - on paper. Type your paper code as-is into a computer. Do as many mock interviews as possible. Practicing implementing the data structures and algorithm (on paper, and then on a computer) is also a great exercise. Make sure to implement it the moment after, and a day later, and even revisit it a week later briefly, to make sure you understand the solution.","title":"General Tips"},{"location":"#mkdocs","text":"For documentation visit mkdocs.org . mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"mkdocs"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"#patterns","text":"14 Patterns to Ace Any Coding Interview Question Leetcode Pattern 0 | Iterative traversals on Trees Leetcode Pattern 1 | BFS + DFS == 25% of the problems \u2014 part 1 Leetcode Pattern 1 | DFS + BFS == 25% of the problems \u2014 part 2 Leetcode Pattern 2 | Sliding Windows for Strings Leetcode Pattern 3 | Backtracking Leetcode Pattern 4 | Meta Stuff","title":"Patterns"},{"location":"about/","text":"Iuncis vulgatum ceu tectus Abluere erat Cytoriaco Lorem markdownum ducebat Iuppiter, pro ante ubi brevi stellis pulchra hunc esse non dumque pereat est. Iam mores potuitque esse. Sonant Anius, opem pastoris, vota num bidentes, pater, illis inexpugnabile primo ego cupiunt sustinet parenti. Crimine essent alto interea piget refert Iacit currus habitabile pascere erimus moenia Fuisset est deum nectar Equorum mergit quinquennia sorores quae forti parentem Leve est nam exercebat ignorant et pede Parentum relinquam in quoque aurea Tuae ubi rursus est figuras timidum et Ieiunia id suis incursat Surgit laeva tibi. Est dare, resupino posuerunt vadit valles, et hanc silendo vocatur mediae. software += restore; pushLanguagePrinter(-5, ergonomics_sd, readerMbr); webPingHashtag *= bar_goodput_compact(icann_atm, uml / 2 - ppi, encoding); php *= rteCrop(copy(583176, link_crt_tag + technology, 34 + jsp)); Inania bene, haec suis geruntur vinaque; facta est . O carcere liquefacta , furit mentoque, accipimus agrestes, has non movetur in carmine poscit simulavit. Esse eris nam Aeginam iacit me dixerunt laevum consule pedes. Audis enim Arcades animalia gemmas Medusae altissima. Et Perseus liquores addita Oblitus puer quae nunc videt tantoque, adspicio Haec, neu in facta ubi per pensandum medio, vertere. Ait acuta palmite humi: ex pudetque temporis Cythereius intremuit auctor. icsEbookWamp /= raidMaster.hard_dvd_drive(lcd_hard_google); osdKernelSequence.heapT = bugDdrSsid.docking(beta_cookie(8)) + shortcut( menu_num_port(1), ppmBarRdf + hertzEthics); var piracy_mouse = readme; if (dot_gbps = 4) { friendly_portal(-5); } else { gate += boxVrmlIo; dvi += trinitronBinAnsi; } In adurat cassis Atridae erubuit et inrita Sicyonius volucrum peperisse sunt Saturnia: cavas. Est ardet, veretur, terribilis situs et veloque omnes Ianthe direptos Dolona. Puellari turpique iam quoque etiam Stygias nec pretium seque, petis mihi forte Sithonios feruntur bisulcam, inquam gestumque iubet. Pigetque quod, nisi capiti nympha interea. Quo nec pependit putares victa eques in duabus illo caput.","title":"Example Page"},{"location":"about/#iuncis-vulgatum-ceu-tectus","text":"","title":"Iuncis vulgatum ceu tectus"},{"location":"about/#abluere-erat-cytoriaco","text":"Lorem markdownum ducebat Iuppiter, pro ante ubi brevi stellis pulchra hunc esse non dumque pereat est. Iam mores potuitque esse. Sonant Anius, opem pastoris, vota num bidentes, pater, illis inexpugnabile primo ego cupiunt sustinet parenti. Crimine essent alto interea piget refert Iacit currus habitabile pascere erimus moenia Fuisset est deum nectar Equorum mergit quinquennia sorores quae forti parentem Leve est nam exercebat ignorant et pede Parentum relinquam in quoque aurea","title":"Abluere erat Cytoriaco"},{"location":"about/#tuae-ubi-rursus-est-figuras-timidum-et","text":"Ieiunia id suis incursat Surgit laeva tibi. Est dare, resupino posuerunt vadit valles, et hanc silendo vocatur mediae. software += restore; pushLanguagePrinter(-5, ergonomics_sd, readerMbr); webPingHashtag *= bar_goodput_compact(icann_atm, uml / 2 - ppi, encoding); php *= rteCrop(copy(583176, link_crt_tag + technology, 34 + jsp)); Inania bene, haec suis geruntur vinaque; facta est . O carcere liquefacta , furit mentoque, accipimus agrestes, has non movetur in carmine poscit simulavit. Esse eris nam Aeginam iacit me dixerunt laevum consule pedes. Audis enim Arcades animalia gemmas Medusae altissima.","title":"Tuae ubi rursus est figuras timidum et"},{"location":"about/#et-perseus-liquores-addita","text":"Oblitus puer quae nunc videt tantoque, adspicio Haec, neu in facta ubi per pensandum medio, vertere. Ait acuta palmite humi: ex pudetque temporis Cythereius intremuit auctor. icsEbookWamp /= raidMaster.hard_dvd_drive(lcd_hard_google); osdKernelSequence.heapT = bugDdrSsid.docking(beta_cookie(8)) + shortcut( menu_num_port(1), ppmBarRdf + hertzEthics); var piracy_mouse = readme; if (dot_gbps = 4) { friendly_portal(-5); } else { gate += boxVrmlIo; dvi += trinitronBinAnsi; } In adurat cassis Atridae erubuit et inrita Sicyonius volucrum peperisse sunt Saturnia: cavas. Est ardet, veretur, terribilis situs et veloque omnes Ianthe direptos Dolona. Puellari turpique iam quoque etiam Stygias nec pretium seque, petis mihi forte Sithonios feruntur bisulcam, inquam gestumque iubet. Pigetque quod, nisi capiti nympha interea. Quo nec pependit putares victa eques in duabus illo caput.","title":"Et Perseus liquores addita"},{"location":"arrays-and-strings/","text":"Array Problems String Problems Please note that array questions and string questions are often interchangeable. Hash Tables A hash table is a data structure that maps keys to values for highly efficient lookup. There are a number of ways of implementing this. A simple but common implementation could use an array of linked lists and a hash code function. # Declare a dictionary. some_dict = {'Name': 'Ben', 'Age': 23} some_other_dict = dict() # Accessing the dictionary with its key. print(some_dict['Name']) # Update existing entry. some_dict['Name'] = 'Bob' # Remove entry. del some_dict['Name'] # Remove all entries. some_dict.clear() # Delete entire dictionary. del some_dict Arraylist Resizable Arrays Lists are resizable by default in Python. names = ['Ben', 'Bob', 'Bill'] some_list = list() for name in names: print(name) Amortized insertion runtime is O(1). StringBuilder some_name = 'Ben' some_name += ' Clauss'","title":"Arrays & Strings"},{"location":"arrays-and-strings/#hash-tables","text":"A hash table is a data structure that maps keys to values for highly efficient lookup. There are a number of ways of implementing this. A simple but common implementation could use an array of linked lists and a hash code function. # Declare a dictionary. some_dict = {'Name': 'Ben', 'Age': 23} some_other_dict = dict() # Accessing the dictionary with its key. print(some_dict['Name']) # Update existing entry. some_dict['Name'] = 'Bob' # Remove entry. del some_dict['Name'] # Remove all entries. some_dict.clear() # Delete entire dictionary. del some_dict","title":"Hash Tables"},{"location":"arrays-and-strings/#arraylist-resizable-arrays","text":"Lists are resizable by default in Python. names = ['Ben', 'Bob', 'Bill'] some_list = list() for name in names: print(name) Amortized insertion runtime is O(1).","title":"Arraylist &amp; Resizable Arrays"},{"location":"arrays-and-strings/#stringbuilder","text":"some_name = 'Ben' some_name += ' Clauss'","title":"StringBuilder"},{"location":"dynamic-programming/","text":"","title":"Dynamic Programming"},{"location":"graphs/","text":"Graph Problems DFS Problems BFS Problems A tree is actually a type of graph, but not all graphs are trees. Simply put, a tree is a connected graph without cycles. A graph is simply a collection of nodes with edges between (some of) them. - Graphs can be either directed (like the following graph) or undirected. - The graph might consist of multiple isolated subgraphs. If there is a path between every pair of vertices, it is called a \"connected graph\". - The graph can also have cycles (or not). An \"acyclic graph\" is one without cycles. Implementation Adjacency List This is the most common way to represent a graph. Every vertex (or node) stores a list of adjacent vertices. In an undirected graph, an edge like (a, b) would be stored twice: once in a 's adjacent vertices and once in b 's adjacent vertices. class Graph: def __init__(self, nodes): self.nodes = nodes class Node: def __init__(self, name, nodes): self.name = name self.nodes = nodes The Graph class is used because, unlike in a tree, you can't necessarily reach all the nodes from a single node. You don't necessarily need any additional classes to represent a graph. An array (or a hash table) of lists (arrays, arraylists, linked lists, etc.) can store the adjacency list. Adjacency Matrices An adjacency matrix is an NxN boolean matrix (where N is the number of nodes), where a True value at matrix[i][j] indicates an edge from node i to node j . (You can also use an integer matrix with O s and 1 s.) In an undirected graph, an adjacency matrix will be symmetric. In a directed graph, it will not (necessarily) be. The same graph algorithms that are used on adjacency lists (breadth-first search, etc.) can be performed with adjacency matrices, but they may be somewhat less efficient. In the adjacency list representation, you can easily iterate through the neighbors of a node. In the adjacency matrix representation, you will need to iterate through all the nodes to identify a node's neighbors. Graph Search The two most common ways to search a graph are depth-first search and breadth-first search. In depth-first search (DFS), we start at the root (or another arbitrarily selected node) and explore each branch completely before moving on to the next branch. That is, we go deep first (hence the name depth\u00ad first search) before we go wide. In breadth-first search (BFS), we start at the root (or another arbitrarily selected node) and explore each neighbor before going on to any of their children. That is, we go wide (hence breadth-first search) before we go deep. Breadth-first search and depth-first search tend to be used in different scenarios. DFS is often preferred if we want to visit every node in the graph. Both will work just fine, but depth-first search is a bit simpler. However, if we want to find the shortest path (or just any path) between two nodes, BFS is generally better. Depth-First Search (DFS) In DFS, we visit a node a and then iterate through each of a 's neighbors. When visiting a node b that is a neighbor of a , we visit all of b 's neighbors before going on to a 's other neighbors. That is, a exhaustively searches b 's branch before any of its other neighbors. Note that pre-order and other forms of tree traversal are a form of DFS. The key difference is that when implementing this algorithm for a graph, we must check if the node has been visited. If we don't, we risk getting stuck in an infinite loop. pass Breadth-First Search (BFS) BFS is a bit less intuitive, and many interviewees struggle with the implementation unless they are already familiar with it. The main tripping point is the (false) assumption that BFS is recursive. It's not. Instead, it uses a queue. In BFS, node a visits each of a 's neighbors before visiting any of their neighbors. You can think of this as searching level by level out from a . An iterative solution involving a queue usually works best. pass If you are asked to implement BFS, the key thing to remember is the use of the queue. The rest of the algo\u00adrithm flows from this fact. Bidirectional Search Bidirectional search is used to find the shortest path between a source and destination node. It operates by essentially running two simultaneous breadth-first searches, one from each node. When their searches collide, we have found a path. To see why this is faster, consider a graph where every node has at most k adjacent nodes and the shortest path from node s to nodet has length d. In traditional breadth-first search, we would search up to k nodes in the first \"level\" of the search. In the second level, we would search up to k nodes for each of those first k nodes, so k2 nodes total (thus far). We would do this d times, so that's 0( kd) nodes. In bidirectional search, we have two searches that collide after approximately \ufffd levels (the midpoint of the path). The search from s visits approximately kd12, as does the search fromt.That's approximately 2 kdl2, or 0( kd/2), nodes total. This might seem like a minor difference, but it's not. It's huge. Recall that tional search is actually faster by a factor of kd12. Put another way: if our system could only support searching \"friend of friend\" paths in breadth-first search, it could now likely support \"friend of friend of friend of friend\" paths. We can support paths that are twice as long.","title":"Graphs"},{"location":"graphs/#implementation","text":"","title":"Implementation"},{"location":"graphs/#adjacency-list","text":"This is the most common way to represent a graph. Every vertex (or node) stores a list of adjacent vertices. In an undirected graph, an edge like (a, b) would be stored twice: once in a 's adjacent vertices and once in b 's adjacent vertices. class Graph: def __init__(self, nodes): self.nodes = nodes class Node: def __init__(self, name, nodes): self.name = name self.nodes = nodes The Graph class is used because, unlike in a tree, you can't necessarily reach all the nodes from a single node. You don't necessarily need any additional classes to represent a graph. An array (or a hash table) of lists (arrays, arraylists, linked lists, etc.) can store the adjacency list.","title":"Adjacency List"},{"location":"graphs/#adjacency-matrices","text":"An adjacency matrix is an NxN boolean matrix (where N is the number of nodes), where a True value at matrix[i][j] indicates an edge from node i to node j . (You can also use an integer matrix with O s and 1 s.) In an undirected graph, an adjacency matrix will be symmetric. In a directed graph, it will not (necessarily) be. The same graph algorithms that are used on adjacency lists (breadth-first search, etc.) can be performed with adjacency matrices, but they may be somewhat less efficient. In the adjacency list representation, you can easily iterate through the neighbors of a node. In the adjacency matrix representation, you will need to iterate through all the nodes to identify a node's neighbors.","title":"Adjacency Matrices"},{"location":"graphs/#graph-search","text":"The two most common ways to search a graph are depth-first search and breadth-first search. In depth-first search (DFS), we start at the root (or another arbitrarily selected node) and explore each branch completely before moving on to the next branch. That is, we go deep first (hence the name depth\u00ad first search) before we go wide. In breadth-first search (BFS), we start at the root (or another arbitrarily selected node) and explore each neighbor before going on to any of their children. That is, we go wide (hence breadth-first search) before we go deep. Breadth-first search and depth-first search tend to be used in different scenarios. DFS is often preferred if we want to visit every node in the graph. Both will work just fine, but depth-first search is a bit simpler. However, if we want to find the shortest path (or just any path) between two nodes, BFS is generally better.","title":"Graph Search"},{"location":"graphs/#depth-first-search-dfs","text":"In DFS, we visit a node a and then iterate through each of a 's neighbors. When visiting a node b that is a neighbor of a , we visit all of b 's neighbors before going on to a 's other neighbors. That is, a exhaustively searches b 's branch before any of its other neighbors. Note that pre-order and other forms of tree traversal are a form of DFS. The key difference is that when implementing this algorithm for a graph, we must check if the node has been visited. If we don't, we risk getting stuck in an infinite loop. pass","title":"Depth-First Search (DFS)"},{"location":"graphs/#breadth-first-search-bfs","text":"BFS is a bit less intuitive, and many interviewees struggle with the implementation unless they are already familiar with it. The main tripping point is the (false) assumption that BFS is recursive. It's not. Instead, it uses a queue. In BFS, node a visits each of a 's neighbors before visiting any of their neighbors. You can think of this as searching level by level out from a . An iterative solution involving a queue usually works best. pass If you are asked to implement BFS, the key thing to remember is the use of the queue. The rest of the algo\u00adrithm flows from this fact.","title":"Breadth-First Search (BFS)"},{"location":"graphs/#bidirectional-search","text":"Bidirectional search is used to find the shortest path between a source and destination node. It operates by essentially running two simultaneous breadth-first searches, one from each node. When their searches collide, we have found a path. To see why this is faster, consider a graph where every node has at most k adjacent nodes and the shortest path from node s to nodet has length d. In traditional breadth-first search, we would search up to k nodes in the first \"level\" of the search. In the second level, we would search up to k nodes for each of those first k nodes, so k2 nodes total (thus far). We would do this d times, so that's 0( kd) nodes. In bidirectional search, we have two searches that collide after approximately \ufffd levels (the midpoint of the path). The search from s visits approximately kd12, as does the search fromt.That's approximately 2 kdl2, or 0( kd/2), nodes total. This might seem like a minor difference, but it's not. It's huge. Recall that tional search is actually faster by a factor of kd12. Put another way: if our system could only support searching \"friend of friend\" paths in breadth-first search, it could now likely support \"friend of friend of friend of friend\" paths. We can support paths that are twice as long.","title":"Bidirectional Search"},{"location":"heaps/","text":"Heap Problems Binary Heaps (Min-Heaps and Max-Heaps) A min-heap is a complete binary tree (that is, totally filled other than the rightmost elements on the last level) where each node is smaller than its children. The root, therefore, is the minimum element in the tree. Insert When we insert into a min-heap, we always start by inserting the element at the bottom. We insert at the rightmost spot so as to maintain the complete tree property. Then, we \"fix\" the tree by swapping the new element with its parent, until we find an appropriate spot for the element. We essentially bubble up the minimum element. This takes O(log n) time, where n is the number of nodes in the heap. Extract Minimum Element Finding the minimum element of a min-heap is easy: it's always at the top. The trickier part is how to remove it. (In fact, this isn't that tricky.) First, we remove the minimum element and swap it with the last element in the heap (the bottommost, rightmost element). Then, we bubble down this element, swapping it with one of its children until the min\u00ad heap property is restored. Do we swap it with the left child or the right child? That depends on their values. There's no inherent ordering between the left and right element, but you'll need to take the smaller one in order to maintain the min-heap ordering. This algorithm will also take O(log n) time. Implementation Usage Note, the default heap implementation is a min-heap. To use a max-heap, invert the value of the keys. Heap elements can also be tuples. import heapq numbers = [5, 7, 9, 1, 3] # Use heapify to convert a list into a heap. heapq.heapify(numbers) # Use heappush() to push elements. heapq.heappush(numbers, 4) # Use heappop() to pop smallest element. print(heapq.heappop(numbers))","title":"Heaps"},{"location":"heaps/#implementation","text":"","title":"Implementation"},{"location":"heaps/#usage","text":"Note, the default heap implementation is a min-heap. To use a max-heap, invert the value of the keys. Heap elements can also be tuples. import heapq numbers = [5, 7, 9, 1, 3] # Use heapify to convert a list into a heap. heapq.heapify(numbers) # Use heappush() to push elements. heapq.heappush(numbers, 4) # Use heappop() to pop smallest element. print(heapq.heappop(numbers))","title":"Usage"},{"location":"linked-lists/","text":"Linked List Problems A linked list is a data structure that represents a sequence of nodes. In a singly linked list, each node points to the next node in the linked list. A doubly linked list gives each node pointers to both the next node and the previous node. Unlike an array, a linked list does not provide constant time access to a particular \"index\" within the list. This means that if you'd like to find the Kth element in the list, you will need to iterate through K elements. The benefit of a linked list is that you can add and remove items from the beginning of the list in constant time. For specific applications, this can be useful. A number of linked list problems rely on recursion. If you're having trouble solving a linked list problem, you should explore if a recursive approach will work. Remember that recursive algorithms take at least O(n) space, where n is the depth of the recursive call. All recursive algorithms can be implemented iteratively, although they may be much more complex. class SingleNode: def __init__(self, value=None): self.value = value self.next = None class SingleLinkedList: def __init__(self): self.head = None def print(self): node = self.head while node is not None: print(node.value) node = node.next class DoubleNode: def __init__(self, value=None): self.value = value self.next = None self.prev = None class DoubleLinkedList: def __init__(self): self.head = None def print(self): node = self.head while node is not None: print(node.value) node = node.next The \"Runner\" Technique The \"runner\" (or second pointer) technique is used in many linked list problems. The runner technique means that you iterate through the linked list with two pointers simultaneously, with one ahead of the other. The \"fast\" node might be ahead by a fixed amount, or it might be hopping multiple nodes for each one node that the \"slow\" node iterates through.","title":"Linked Lists"},{"location":"linked-lists/#the-runner-technique","text":"The \"runner\" (or second pointer) technique is used in many linked list problems. The runner technique means that you iterate through the linked list with two pointers simultaneously, with one ahead of the other. The \"fast\" node might be ahead by a fixed amount, or it might be hopping multiple nodes for each one node that the \"slow\" node iterates through.","title":"The \"Runner\" Technique"},{"location":"queues/","text":"Queue Problems A queue implements FIFO (first-in first-out) ordering. As in a line or queue at a ticket stand, items are removed from the data structure in the same order that they are added. It uses the operations: add(item) : Add an item to the end of the list. remove() : Remove the first item in the list. peek() : Return the top of the queue. is_empty() : Return True if and only if the queue is empty. A queue can also be implemented with a linked list. In fact, they are essentially the same thing, as long as items are added and removed from opposite sides. Implementation class Queue: def __init__(self): self.queue = [] def add(self, item): self.queue.append(item) def remove(self): if len(self.queue) 0: return self.queue.pop(0) return None def peek(self): if len(self.queue) 0: return self.queue[0] return None def is_empty(self): return len(self.queue) == 0 It is especially easy to mess up the updating of the first and last nodes in a queue. Be sure to double check this. One place where queues are often used is in breadth-first search or in implementing a cache. In breadth-first search, for example, we used a queue to store a list of the nodes that we need to process. Each time we process a node, we add its adjacent nodes to the back of the queue. This allows us to process nodes in the order in which they are viewed. Usage List fruits = [] # Add to queue. fruits.append('banana') fruits.append('grapes') # Remove from queue. first_item = fruits.pop(0) deque from collections import deque numbers = deque() numbers.append(99) numbers.append(15) last_item = numbers.popleft()","title":"Queues"},{"location":"queues/#implementation","text":"class Queue: def __init__(self): self.queue = [] def add(self, item): self.queue.append(item) def remove(self): if len(self.queue) 0: return self.queue.pop(0) return None def peek(self): if len(self.queue) 0: return self.queue[0] return None def is_empty(self): return len(self.queue) == 0 It is especially easy to mess up the updating of the first and last nodes in a queue. Be sure to double check this. One place where queues are often used is in breadth-first search or in implementing a cache. In breadth-first search, for example, we used a queue to store a list of the nodes that we need to process. Each time we process a node, we add its adjacent nodes to the back of the queue. This allows us to process nodes in the order in which they are viewed.","title":"Implementation"},{"location":"queues/#usage","text":"List fruits = [] # Add to queue. fruits.append('banana') fruits.append('grapes') # Remove from queue. first_item = fruits.pop(0) deque from collections import deque numbers = deque() numbers.append(99) numbers.append(15) last_item = numbers.popleft()","title":"Usage"},{"location":"search-and-sort/","text":"Binary Search def binary_search(array, target): n = len(array) low = 0 high = n - 1 while low = high: mid = (low + high) // 2 if array[mid] target: low = mid + 1 elif array[mid] target: high = mid - 1 else: return mid return -1","title":"Search & Sort Algorithms"},{"location":"search-and-sort/#binary-search","text":"def binary_search(array, target): n = len(array) low = 0 high = n - 1 while low = high: mid = (low + high) // 2 if array[mid] target: low = mid + 1 elif array[mid] target: high = mid - 1 else: return mid return -1","title":"Binary Search"},{"location":"stacks/","text":"Stack Problems The stack data structure is precisely what it sounds like: a stack of data. In certain types of problems, it can be favorable to store data in a stack rather than in an array. A stack uses LIFO (last-in first-out) ordering. That is, as in a stack of dinner plates, the most recent item added to the stack is the first item to be removed. It uses the following operations: pop() : Remove the top item from the stack. push(item) : Add an item to the top of the stack. peek() : Return the top of the stack. is_empty() : Return True if and only if the stack is empty. Implementation class Stack: def __init__(self): self.stack = [] def pop(self): if len(self.stack) 0: return self.stack.pop() return None def push(self, item): self.stack.append(item) def peek(self): if len(self.stack) 0: return self.stack[len(self.stack) - 1] return None def is_empty(self): return len(self.stack) == 0 One case where stacks are often useful is in certain recursive algorithms. Sometimes you need to push temporary data onto a stack as you recurse, but then remove them as you backtrack (for example, because the recursive check failed). A stack offers an intuitive way to do this. A stack can also be used to implement a recursive algorithm iteratively. (This is a good exercise! Take a simple recursive algorithm and implement it iteratively.) Usage List letters = [] # Add to stack. letters.append('a') letters.append('b') # Remove from stack. last_item = letters.pop() deque from collections import deque numbers = deque() # Use append like before to add elements. numbers.append(99) numbers.append(15) # You can pop like a stack. last_item = numbers.pop()","title":"Stacks"},{"location":"stacks/#implementation","text":"class Stack: def __init__(self): self.stack = [] def pop(self): if len(self.stack) 0: return self.stack.pop() return None def push(self, item): self.stack.append(item) def peek(self): if len(self.stack) 0: return self.stack[len(self.stack) - 1] return None def is_empty(self): return len(self.stack) == 0 One case where stacks are often useful is in certain recursive algorithms. Sometimes you need to push temporary data onto a stack as you recurse, but then remove them as you backtrack (for example, because the recursive check failed). A stack offers an intuitive way to do this. A stack can also be used to implement a recursive algorithm iteratively. (This is a good exercise! Take a simple recursive algorithm and implement it iteratively.)","title":"Implementation"},{"location":"stacks/#usage","text":"List letters = [] # Add to stack. letters.append('a') letters.append('b') # Remove from stack. last_item = letters.pop() deque from collections import deque numbers = deque() # Use append like before to add elements. numbers.append(99) numbers.append(15) # You can pop like a stack. last_item = numbers.pop()","title":"Usage"},{"location":"trees/","text":"Tree Problems Binary Search Tree Problems Types of Trees A nice way to understand a tree is with a recursive explanation. A tree is a data structure composed of nodes. - Each tree has a root node. (Actually, this isn't strictly necessary in graph theory, but it's usually how we use trees in programming, and especially programming interviews.) - The root node has zero or more child nodes. - Each child node has zero or more child nodes, and so on. The tree cannot contain cycles. The nodes may or may not be in a particular order, they could have any data type as values, and they may or may not have links back to their parent nodes. Implementation class TreeNode: def __init__(self, value=None, children=[]): self.value = value self.children = children class BinaryTreeNode: def __init__(self, value=0, left=None, right=None): self.value = value self.left = left self.right = right Trees vs. Binary Trees A binary tree is a tree in which each node has up to two children. Not all trees are binary trees. There are occasions when you might have a tree that is not a binary tree. For example, suppose you were using a tree to represent a bunch of phone numbers. In this case, you might use a 10-ary tree, with each node having up to 10 children (one for each digit). A node is called a \"leaf\" node if it has no children. Binary Tree vs. Binary Search Tree A binary search tree is a binary tree in which every node fits a specific ordering property: all left descendents = n all right descendents .This must be true for each node n . The definition of a binary search tree can vary slightly with respect to equality. Under some defi\u00adnitions, the tree cannot have duplicate values. In others, the duplicate values will be on the right or can be on either side. All are valid definitions, but you should clarify this with your interviewer. When given a tree question, many candidates assume the interviewer means a binary search tree. Be sure to ask. A binary search tree imposes the condition that, for each node, its left descendents are less than or equal to the current node, which is less than the right descendents. Balanced vs. Unbalanced While many trees are balanced, not all are. Ask your interviewer for clarification here. Note that balancing a tree does not mean the left and right subtrees are exactly the same size. Complete Binary Trees A complete binary tree is a binary tree in which every level of the tree is fully filled, except for perhaps the last level. To the extent that the last level is filled, it is filled left to right. Full Binary Trees A full binary tree is a binary tree in which every node has either zero or two children. That is, no nodes have only one child. Perfect Binary Trees A perfect binary tree is one that is both full and complete. All leaf nodes will be at the same level, and this level has the maximum number of nodes. Note that perfect trees are rare in interviews and in real life, as a perfect tree must have exactly 2^k - 1 nodes (where k is the number of levels). In an interview, do not assume a binary tree is perfect. Binary Tree Traversal Prior to your interview, you should be comfortable implementing in-order, post-order, and pre-order traversal. The most common of these is in-order traversal. In-Order Traversal In-order traversal means to \"visit\" (often, print) the left branch, then the current node, and finally, the right branch. def recursive_in_order_traversal(node): if node != None: in_order_traversal(node.left) print(node.value) in_order_traversal(node.right) def iterative_in_order_traversal(root): if root is None: return current = root stack = [] while True: if current is not None: stack.append(current) current = current.left elif len(stack) 0: current = stack.pop() print(node.value) current = current.right else: break When performed on a binary search tree, it visits the nodes in ascending order (hence the name \"in-order\"). Pre-Order Traversal Pre-order traversal visits the current node before its child nodes (hence the name \"pre-order\"). def recursive_pre_order_traversal(node): if node != None: print(node.value) pre_order_traversal(node.left) pre_order_traversal(node.right) def iterative_pre_order_traversal(root): if root is None: return stack = [] stack.append(root) while len(stack) 0: node = stack.pop() print(node.value) if node.right is not None: stack.append(node.right) if node.left is not None: stack.append(node.left) In a pre-order traversal, the root is always the first node visited. Post-Order Traversal Post-order traversal visits the current node after its child nodes (hence the name\"post-order\"). def post_order_traversal(node): if node != None: post_order_traversal(node.left) post_order_traversal(node.right) print(node.value) # TODO: Review and test this. def iterative_post_order_traversal(root): if root is None: return stack = [] while(True): while (root): if root.right is not None: stack.append(root.right) stack.append(root) root = root.left root = stack.pop() if (root.right is not None and peek(stack) == root.right): stack.pop() stack.append(root) root = root.right else: ans.append(root.value) root = None if len(stack) = 0: break In a post-order traversal, the root is always the last node visited.","title":"Trees"},{"location":"trees/#implementation","text":"class TreeNode: def __init__(self, value=None, children=[]): self.value = value self.children = children class BinaryTreeNode: def __init__(self, value=0, left=None, right=None): self.value = value self.left = left self.right = right Trees vs. Binary Trees A binary tree is a tree in which each node has up to two children. Not all trees are binary trees. There are occasions when you might have a tree that is not a binary tree. For example, suppose you were using a tree to represent a bunch of phone numbers. In this case, you might use a 10-ary tree, with each node having up to 10 children (one for each digit). A node is called a \"leaf\" node if it has no children. Binary Tree vs. Binary Search Tree A binary search tree is a binary tree in which every node fits a specific ordering property: all left descendents = n all right descendents .This must be true for each node n . The definition of a binary search tree can vary slightly with respect to equality. Under some defi\u00adnitions, the tree cannot have duplicate values. In others, the duplicate values will be on the right or can be on either side. All are valid definitions, but you should clarify this with your interviewer. When given a tree question, many candidates assume the interviewer means a binary search tree. Be sure to ask. A binary search tree imposes the condition that, for each node, its left descendents are less than or equal to the current node, which is less than the right descendents. Balanced vs. Unbalanced While many trees are balanced, not all are. Ask your interviewer for clarification here. Note that balancing a tree does not mean the left and right subtrees are exactly the same size. Complete Binary Trees A complete binary tree is a binary tree in which every level of the tree is fully filled, except for perhaps the last level. To the extent that the last level is filled, it is filled left to right. Full Binary Trees A full binary tree is a binary tree in which every node has either zero or two children. That is, no nodes have only one child. Perfect Binary Trees A perfect binary tree is one that is both full and complete. All leaf nodes will be at the same level, and this level has the maximum number of nodes. Note that perfect trees are rare in interviews and in real life, as a perfect tree must have exactly 2^k - 1 nodes (where k is the number of levels). In an interview, do not assume a binary tree is perfect.","title":"Implementation"},{"location":"trees/#binary-tree-traversal","text":"Prior to your interview, you should be comfortable implementing in-order, post-order, and pre-order traversal. The most common of these is in-order traversal. In-Order Traversal In-order traversal means to \"visit\" (often, print) the left branch, then the current node, and finally, the right branch. def recursive_in_order_traversal(node): if node != None: in_order_traversal(node.left) print(node.value) in_order_traversal(node.right) def iterative_in_order_traversal(root): if root is None: return current = root stack = [] while True: if current is not None: stack.append(current) current = current.left elif len(stack) 0: current = stack.pop() print(node.value) current = current.right else: break When performed on a binary search tree, it visits the nodes in ascending order (hence the name \"in-order\"). Pre-Order Traversal Pre-order traversal visits the current node before its child nodes (hence the name \"pre-order\"). def recursive_pre_order_traversal(node): if node != None: print(node.value) pre_order_traversal(node.left) pre_order_traversal(node.right) def iterative_pre_order_traversal(root): if root is None: return stack = [] stack.append(root) while len(stack) 0: node = stack.pop() print(node.value) if node.right is not None: stack.append(node.right) if node.left is not None: stack.append(node.left) In a pre-order traversal, the root is always the first node visited. Post-Order Traversal Post-order traversal visits the current node after its child nodes (hence the name\"post-order\"). def post_order_traversal(node): if node != None: post_order_traversal(node.left) post_order_traversal(node.right) print(node.value) # TODO: Review and test this. def iterative_post_order_traversal(root): if root is None: return stack = [] while(True): while (root): if root.right is not None: stack.append(root.right) stack.append(root) root = root.left root = stack.pop() if (root.right is not None and peek(stack) == root.right): stack.pop() stack.append(root) root = root.right else: ans.append(root.value) root = None if len(stack) = 0: break In a post-order traversal, the root is always the last node visited.","title":"Binary Tree Traversal"},{"location":"tries/","text":"Trie Problems A trie is a variant of an n-ary tree in which characters are stored at each node. Each path down the tree may repr The * nodes (sometimes called \"null nodes\") are often used to indicate complete words. The actual implementation of these * nodes might be a special type of child (such as a TerminatingTrieNode , which inherits from TrieNode ). Or, we could use just a boolean flag terminates within the \"parent\" node. A node in a trie could have anywhere from 1 through ALPHABET_SIZE + 1 children (or, 0 through ALPHABET_SIZE if a boolean flag is used instead of a * node). Very commonly, a trie is used to store the entire (English) language for quick prefix lookups. While a hash table can quickly look up whether a string is a valid word, it cannot tell us if a string is a prefix of any valid words. A trie can do this very quickly. How quickly? A trie can check if a string is a valid prefix in O(K) time, where K is the length of the string. This is actually the same runtime as a hash table will take. Although we often refer to hash table lookups as being O(1) time, this isn't entirely true. A hash table must read through all the characters in the input, which takes O(K) time in the case of a word lookup. Implementation # TODO: ... Usage # TODO: ...","title":"Tries (Prefix Trees)"},{"location":"tries/#implementation","text":"# TODO: ...","title":"Implementation"},{"location":"tries/#usage","text":"# TODO: ...","title":"Usage"}]}