{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"General Tips Try to solve the problem on your own. Write the code on paper. Test your code - on paper. Type your paper code as-is into a computer. Check your code. Test your code. If your code works, do not immediately look at a solution. Try to optimize your code. Review discussion tab and document findings. When reviewing a solution for a problem you struggle with, make sure to implement it the moment after, and a day later, and even revisit it a week later briefly, to make sure you understand the solution. Do as many mock interviews as possible. Practicing implementing the data structures and algorithm (on paper, and then on a computer) is also a great exercise. Patterns 14 Patterns to Ace Any Coding Interview Question Leetcode Pattern 0 | Iterative traversals on Trees Leetcode Pattern 1 | BFS + DFS == 25% of the problems \u2014 part 1 Leetcode Pattern 1 | DFS + BFS == 25% of the problems \u2014 part 2 Leetcode Pattern 2 | Sliding Windows for Strings Leetcode Pattern 3 | Backtracking Leetcode Pattern 4 | Meta Stuff LeetCode Topics This is a list of problem tags on LeetCode and where to find more information on how to solve those problems here. Topic Page Hash Table Arrays and Strings Linked List Linked Lists Math Not Done Two Pointers Linked Lists String Not Done Binary Search Not Done Divide and Conquer Recursion and Dynamic Programming Dynamic Programming Recursion and Dynamic Programming Stack Backtracking Backtracking Heap Heaps Greedy Not Done Sort Not Done Bit Manipulation Not Done Tree Trees DFS BFS Union Find Not Done Graph Graphs Design Not Done Topological Sort Graphs Trie Tries Binary Indexed Tree Not Done Segment Tree Not Done Binary Search Tree Not Done Recursion Not Done Brainteaser Not Done Memoization Recursion and Dynamic Programming Minimax Not Done Reservoir Sampling Not Done Ordered Map Not Done Geometry Not Done Random Not Done Rejection Sampling Not Done Sliding Window Not Done Line Sweep Not Done Rolling Hash Not Done Suffix Array Not Done mkdocs For documentation visit mkdocs.org . mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#general-tips","text":"Try to solve the problem on your own. Write the code on paper. Test your code - on paper. Type your paper code as-is into a computer. Check your code. Test your code. If your code works, do not immediately look at a solution. Try to optimize your code. Review discussion tab and document findings. When reviewing a solution for a problem you struggle with, make sure to implement it the moment after, and a day later, and even revisit it a week later briefly, to make sure you understand the solution. Do as many mock interviews as possible. Practicing implementing the data structures and algorithm (on paper, and then on a computer) is also a great exercise.","title":"General Tips"},{"location":"#patterns","text":"14 Patterns to Ace Any Coding Interview Question Leetcode Pattern 0 | Iterative traversals on Trees Leetcode Pattern 1 | BFS + DFS == 25% of the problems \u2014 part 1 Leetcode Pattern 1 | DFS + BFS == 25% of the problems \u2014 part 2 Leetcode Pattern 2 | Sliding Windows for Strings Leetcode Pattern 3 | Backtracking Leetcode Pattern 4 | Meta Stuff","title":"Patterns"},{"location":"#leetcode-topics","text":"This is a list of problem tags on LeetCode and where to find more information on how to solve those problems here. Topic Page Hash Table Arrays and Strings Linked List Linked Lists Math Not Done Two Pointers Linked Lists String Not Done Binary Search Not Done Divide and Conquer Recursion and Dynamic Programming Dynamic Programming Recursion and Dynamic Programming Stack Backtracking Backtracking Heap Heaps Greedy Not Done Sort Not Done Bit Manipulation Not Done Tree Trees DFS BFS Union Find Not Done Graph Graphs Design Not Done Topological Sort Graphs Trie Tries Binary Indexed Tree Not Done Segment Tree Not Done Binary Search Tree Not Done Recursion Not Done Brainteaser Not Done Memoization Recursion and Dynamic Programming Minimax Not Done Reservoir Sampling Not Done Ordered Map Not Done Geometry Not Done Random Not Done Rejection Sampling Not Done Sliding Window Not Done Line Sweep Not Done Rolling Hash Not Done Suffix Array Not Done","title":"LeetCode Topics"},{"location":"#mkdocs","text":"For documentation visit mkdocs.org . mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"mkdocs"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"about/","text":"Iuncis vulgatum ceu tectus Abluere erat Cytoriaco Lorem markdownum ducebat Iuppiter, pro ante ubi brevi stellis pulchra hunc esse non dumque pereat est. Iam mores potuitque esse. Sonant Anius, opem pastoris, vota num bidentes, pater, illis inexpugnabile primo ego cupiunt sustinet parenti. Crimine essent alto interea piget refert Iacit currus habitabile pascere erimus moenia Fuisset est deum nectar Equorum mergit quinquennia sorores quae forti parentem Leve est nam exercebat ignorant et pede Parentum relinquam in quoque aurea Tuae ubi rursus est figuras timidum et Ieiunia id suis incursat Surgit laeva tibi. Est dare, resupino posuerunt vadit valles, et hanc silendo vocatur mediae. software += restore; pushLanguagePrinter(-5, ergonomics_sd, readerMbr); webPingHashtag *= bar_goodput_compact(icann_atm, uml / 2 - ppi, encoding); php *= rteCrop(copy(583176, link_crt_tag + technology, 34 + jsp)); Inania bene, haec suis geruntur vinaque; facta est . O carcere liquefacta , furit mentoque, accipimus agrestes, has non movetur in carmine poscit simulavit. Esse eris nam Aeginam iacit me dixerunt laevum consule pedes. Audis enim Arcades animalia gemmas Medusae altissima. Et Perseus liquores addita Oblitus puer quae nunc videt tantoque, adspicio Haec, neu in facta ubi per pensandum medio, vertere. Ait acuta palmite humi: ex pudetque temporis Cythereius intremuit auctor. icsEbookWamp /= raidMaster.hard_dvd_drive(lcd_hard_google); osdKernelSequence.heapT = bugDdrSsid.docking(beta_cookie(8)) + shortcut( menu_num_port(1), ppmBarRdf + hertzEthics); var piracy_mouse = readme; if (dot_gbps = 4) { friendly_portal(-5); } else { gate += boxVrmlIo; dvi += trinitronBinAnsi; } In adurat cassis Atridae erubuit et inrita Sicyonius volucrum peperisse sunt Saturnia: cavas. Est ardet, veretur, terribilis situs et veloque omnes Ianthe direptos Dolona. Puellari turpique iam quoque etiam Stygias nec pretium seque, petis mihi forte Sithonios feruntur bisulcam, inquam gestumque iubet. Pigetque quod, nisi capiti nympha interea. Quo nec pependit putares victa eques in duabus illo caput.","title":"Example Page"},{"location":"about/#iuncis-vulgatum-ceu-tectus","text":"","title":"Iuncis vulgatum ceu tectus"},{"location":"about/#abluere-erat-cytoriaco","text":"Lorem markdownum ducebat Iuppiter, pro ante ubi brevi stellis pulchra hunc esse non dumque pereat est. Iam mores potuitque esse. Sonant Anius, opem pastoris, vota num bidentes, pater, illis inexpugnabile primo ego cupiunt sustinet parenti. Crimine essent alto interea piget refert Iacit currus habitabile pascere erimus moenia Fuisset est deum nectar Equorum mergit quinquennia sorores quae forti parentem Leve est nam exercebat ignorant et pede Parentum relinquam in quoque aurea","title":"Abluere erat Cytoriaco"},{"location":"about/#tuae-ubi-rursus-est-figuras-timidum-et","text":"Ieiunia id suis incursat Surgit laeva tibi. Est dare, resupino posuerunt vadit valles, et hanc silendo vocatur mediae. software += restore; pushLanguagePrinter(-5, ergonomics_sd, readerMbr); webPingHashtag *= bar_goodput_compact(icann_atm, uml / 2 - ppi, encoding); php *= rteCrop(copy(583176, link_crt_tag + technology, 34 + jsp)); Inania bene, haec suis geruntur vinaque; facta est . O carcere liquefacta , furit mentoque, accipimus agrestes, has non movetur in carmine poscit simulavit. Esse eris nam Aeginam iacit me dixerunt laevum consule pedes. Audis enim Arcades animalia gemmas Medusae altissima.","title":"Tuae ubi rursus est figuras timidum et"},{"location":"about/#et-perseus-liquores-addita","text":"Oblitus puer quae nunc videt tantoque, adspicio Haec, neu in facta ubi per pensandum medio, vertere. Ait acuta palmite humi: ex pudetque temporis Cythereius intremuit auctor. icsEbookWamp /= raidMaster.hard_dvd_drive(lcd_hard_google); osdKernelSequence.heapT = bugDdrSsid.docking(beta_cookie(8)) + shortcut( menu_num_port(1), ppmBarRdf + hertzEthics); var piracy_mouse = readme; if (dot_gbps = 4) { friendly_portal(-5); } else { gate += boxVrmlIo; dvi += trinitronBinAnsi; } In adurat cassis Atridae erubuit et inrita Sicyonius volucrum peperisse sunt Saturnia: cavas. Est ardet, veretur, terribilis situs et veloque omnes Ianthe direptos Dolona. Puellari turpique iam quoque etiam Stygias nec pretium seque, petis mihi forte Sithonios feruntur bisulcam, inquam gestumque iubet. Pigetque quod, nisi capiti nympha interea. Quo nec pependit putares victa eques in duabus illo caput.","title":"Et Perseus liquores addita"},{"location":"arrays-and-strings/","text":"Array Problems String Problems Sliding Window Problems Window Sliding Technique Please note that array questions and string questions are often interchangeable. Generating all possible substrings in a string of length n would be O(n^2) . Time complexity: O(n^n) . Consider the worst case where s = \"aaaaaaa\" and every prefix of s is present in the dictionary of words, then the recursion tree can grow upto n^n . Hash Tables A hash table is a data structure that maps keys to values for highly efficient lookup. There are a number of ways of implementing this. A simple but common implementation could use an array of linked lists and a hash code function. Usage Use a dictionary. some_dictionary = {} # Add a value. some_dictionary[key] = value # Remove a value. del some_dictionary[key] # Iterate through dictionary. for key in some_dictionary: print(key) # Iterate through items. for key, value in some_dictionary.items(): print(key) print(value) keys = some_dictionary.keys() values = some_dictionary.values() Arraylist Resizable Arrays Lists are resizable by default in Python. For resizable lists (i.e. ArrayList in Java), amortized insertion runtime is O(1) . In the worst case, you have O(n) work copying over all n elements. Most of the time, you have O(1) work.","title":"Arrays & Strings"},{"location":"arrays-and-strings/#hash-tables","text":"A hash table is a data structure that maps keys to values for highly efficient lookup. There are a number of ways of implementing this. A simple but common implementation could use an array of linked lists and a hash code function.","title":"Hash Tables"},{"location":"arrays-and-strings/#usage","text":"Use a dictionary. some_dictionary = {} # Add a value. some_dictionary[key] = value # Remove a value. del some_dictionary[key] # Iterate through dictionary. for key in some_dictionary: print(key) # Iterate through items. for key, value in some_dictionary.items(): print(key) print(value) keys = some_dictionary.keys() values = some_dictionary.values()","title":"Usage"},{"location":"arrays-and-strings/#arraylist-resizable-arrays","text":"Lists are resizable by default in Python. For resizable lists (i.e. ArrayList in Java), amortized insertion runtime is O(1) . In the worst case, you have O(n) work copying over all n elements. Most of the time, you have O(1) work.","title":"Arraylist &amp; Resizable Arrays"},{"location":"backtracing/","text":"Backtracing Problems https://www.youtube.com/watch?v=gBC_Fd8EE8A","title":"Backtracing"},{"location":"bit-manipulation/","text":"Bit manipulation is used in a variety of problems. Sometimes, the question explicitly calls for bit manipu\u00ad lation. Other times, it's simply a useful technique to optimize your code. You should be comfortable doing bit manipulation by hand, as well as with code. Be careful; it's easy to make little mistakes.","title":"Bit Manipulation"},{"location":"c-and-cpp/","text":"","title":"C & C++"},{"location":"graphs/","text":"Graph Problems DFS Problems BFS Problems Topological Sort Tips How does your code handle cycles? Is the graph connected? Is the graph cyclic? A tree is actually a type of graph, but not all graphs are trees. Simply put, a tree is a connected graph without cycles. A graph is simply a collection of nodes with edges between (some of) them. - Graphs can be either directed (like the following graph) or undirected. - The graph might consist of multiple isolated subgraphs. If there is a path between every pair of vertices, it is called a \"connected graph\". - The graph can also have cycles (or not). An \"acyclic graph\" is one without cycles. Implementation Adjacency List This is the most common way to represent a graph. Every vertex (or node) stores a list of adjacent vertices. In an undirected graph, an edge like (a, b) would be stored twice: once in a 's adjacent vertices and once in b 's adjacent vertices. class Graph: def __init__(self, nodes): self.nodes = nodes class Node: def __init__(self, name, nodes): self.name = name self.nodes = nodes The Graph class is used because, unlike in a tree, you can't necessarily reach all the nodes from a single node. You don't necessarily need any additional classes to represent a graph. An array (or a hash table) of lists (arrays, arraylists, linked lists, etc.) can store the adjacency list. Adjacency Matrices An adjacency matrix is an NxN boolean matrix (where N is the number of nodes), where a True value at matrix[i][j] indicates an edge from node i to node j . (You can also use an integer matrix with O s and 1 s.) In an undirected graph, an adjacency matrix will be symmetric. In a directed graph, it will not (necessarily) be. The same graph algorithms that are used on adjacency lists (breadth-first search, etc.) can be performed with adjacency matrices, but they may be somewhat less efficient. In the adjacency list representation, you can easily iterate through the neighbors of a node. In the adjacency matrix representation, you will need to iterate through all the nodes to identify a node's neighbors. Graph Search The two most common ways to search a graph are depth-first search and breadth-first search. In depth-first search (DFS), we start at the root (or another arbitrarily selected node) and explore each branch completely before moving on to the next branch. That is, we go deep first (hence the name depth\u00ad first search) before we go wide. In breadth-first search (BFS), we start at the root (or another arbitrarily selected node) and explore each neighbor before going on to any of their children. That is, we go wide (hence breadth-first search) before we go deep. Breadth-first search and depth-first search tend to be used in different scenarios. DFS is often preferred if we want to visit every node in the graph. Both will work just fine, but depth-first search is a bit simpler. However, if we want to find the shortest path (or just any path) between two nodes, BFS is generally better. Depth-First Search (DFS) In DFS, we visit a node a and then iterate through each of a 's neighbors. When visiting a node b that is a neighbor of a , we visit all of b 's neighbors before going on to a 's other neighbors. That is, a exhaustively searches b 's branch before any of its other neighbors. Note that pre-order and other forms of tree traversal are a form of DFS. The key difference is that when implementing this algorithm for a graph, we must check if the node has been visited. If we don't, we risk getting stuck in an infinite loop. In theoretical computer science, DFS is typically used to traverse an entire graph, and takes time O(|V| + |E|) , linear in the size of the graph. In these applications it also uses space O(|V|) in the worst case to store the stack of vertices on the current search path as well as the set of already-visited vertices. def search(root): if root is None: return visit(root) root.visited = True for neighbor in root.neighbors: if not neighbor.visited: search(neighbor) Breadth-First Search (BFS) BFS is a bit less intuitive, and many interviewees struggle with the implementation unless they are already familiar with it. The main tripping point is the (false) assumption that BFS is recursive. It's not. Instead, it uses a queue. In BFS, node a visits each of a 's neighbors before visiting any of their neighbors. You can think of this as searching level by level out from a . An iterative solution involving a queue usually works best. The time complexity can be expressed as O(|V| + |E|) , since every vertex and every edge will be explored in the worst case. |V| is the number of vertices and |E| is the number of edges in the graph. Note that O(|E|) may vary between O(1) and O(|V|^2) , depending on how sparse the input graph is. def search(root): queue = [] root.marked = True queue.append(root) while len(queue) 0: node = queue.pop(0) visit(node) for neighbor in node.neighbors: if not neighbor.visited: queue.append(neighbor) If you are asked to implement BFS, the key thing to remember is the use of the queue. The rest of the algo\u00adrithm flows from this fact. Bidirectional Search Bidirectional search is used to find the shortest path between a source and destination node. It operates by essentially running two simultaneous breadth-first searches, one from each node. When their searches collide, we have found a path. To see why this is faster, consider a graph where every node has at most k adjacent nodes and the shortest path from node s to nodet has length d. In traditional breadth-first search, we would search up to k nodes in the first \"level\" of the search. In the second level, we would search up to k nodes for each of those first k nodes, so k2 nodes total (thus far). We would do this d times, so that's 0( kd) nodes. In bidirectional search, we have two searches that collide after approximately \ufffd levels (the midpoint of the path). The search from s visits approximately kd12, as does the search fromt.That's approximately 2 kdl2, or 0( kd/2), nodes total. This might seem like a minor difference, but it's not. It's huge. Recall that tional search is actually faster by a factor of kd12. Put another way: if our system could only support searching \"friend of friend\" paths in breadth-first search, it could now likely support \"friend of friend of friend of friend\" paths. We can support paths that are twice as long. Topological Sorting In computer science, a topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v , u comes before v in the ordering. For instance, the vertices of the graph may represent tasks to be performed, and the edges may represent constraints that one task must be performed before another; in this application, a topological ordering is just a valid sequence for the tasks. A topological ordering is possible if and only if the graph has no directed cycles, that is, if it is a directed acyclic graph (DAG). Any DAG has at least one topological ordering, and algorithms are known for constructing a topological ordering of any DAG in linear time. YouTube DFS An alternative algorithm for topological sorting is based on depth-first search. The algorithm loops through each node of the graph, in an arbitrary order, initiating a depth-first search that terminates when it hits any node that has already been visited since the beginning of the topological sort or the node has no outgoing edges (i.e. a leaf node). Since each edge and node is visited once, the algorithm runs in linear time. L \u2190 Empty list that will contain the sorted nodes while exists nodes without a permanent mark do select an unmarked node n visit(n) function visit(node n) if n has a permanent mark then return if n has a temporary mark then stop (not a DAG) mark n with a temporary mark for each node m with an edge from n to m do visit(m) remove temporary mark from n mark n with a permanent mark add n to head of L","title":"Graphs"},{"location":"graphs/#tips","text":"How does your code handle cycles? Is the graph connected? Is the graph cyclic? A tree is actually a type of graph, but not all graphs are trees. Simply put, a tree is a connected graph without cycles. A graph is simply a collection of nodes with edges between (some of) them. - Graphs can be either directed (like the following graph) or undirected. - The graph might consist of multiple isolated subgraphs. If there is a path between every pair of vertices, it is called a \"connected graph\". - The graph can also have cycles (or not). An \"acyclic graph\" is one without cycles.","title":"Tips"},{"location":"graphs/#implementation","text":"Adjacency List This is the most common way to represent a graph. Every vertex (or node) stores a list of adjacent vertices. In an undirected graph, an edge like (a, b) would be stored twice: once in a 's adjacent vertices and once in b 's adjacent vertices. class Graph: def __init__(self, nodes): self.nodes = nodes class Node: def __init__(self, name, nodes): self.name = name self.nodes = nodes The Graph class is used because, unlike in a tree, you can't necessarily reach all the nodes from a single node. You don't necessarily need any additional classes to represent a graph. An array (or a hash table) of lists (arrays, arraylists, linked lists, etc.) can store the adjacency list. Adjacency Matrices An adjacency matrix is an NxN boolean matrix (where N is the number of nodes), where a True value at matrix[i][j] indicates an edge from node i to node j . (You can also use an integer matrix with O s and 1 s.) In an undirected graph, an adjacency matrix will be symmetric. In a directed graph, it will not (necessarily) be. The same graph algorithms that are used on adjacency lists (breadth-first search, etc.) can be performed with adjacency matrices, but they may be somewhat less efficient. In the adjacency list representation, you can easily iterate through the neighbors of a node. In the adjacency matrix representation, you will need to iterate through all the nodes to identify a node's neighbors.","title":"Implementation"},{"location":"graphs/#graph-search","text":"The two most common ways to search a graph are depth-first search and breadth-first search. In depth-first search (DFS), we start at the root (or another arbitrarily selected node) and explore each branch completely before moving on to the next branch. That is, we go deep first (hence the name depth\u00ad first search) before we go wide. In breadth-first search (BFS), we start at the root (or another arbitrarily selected node) and explore each neighbor before going on to any of their children. That is, we go wide (hence breadth-first search) before we go deep. Breadth-first search and depth-first search tend to be used in different scenarios. DFS is often preferred if we want to visit every node in the graph. Both will work just fine, but depth-first search is a bit simpler. However, if we want to find the shortest path (or just any path) between two nodes, BFS is generally better.","title":"Graph Search"},{"location":"graphs/#depth-first-search-dfs","text":"In DFS, we visit a node a and then iterate through each of a 's neighbors. When visiting a node b that is a neighbor of a , we visit all of b 's neighbors before going on to a 's other neighbors. That is, a exhaustively searches b 's branch before any of its other neighbors. Note that pre-order and other forms of tree traversal are a form of DFS. The key difference is that when implementing this algorithm for a graph, we must check if the node has been visited. If we don't, we risk getting stuck in an infinite loop. In theoretical computer science, DFS is typically used to traverse an entire graph, and takes time O(|V| + |E|) , linear in the size of the graph. In these applications it also uses space O(|V|) in the worst case to store the stack of vertices on the current search path as well as the set of already-visited vertices. def search(root): if root is None: return visit(root) root.visited = True for neighbor in root.neighbors: if not neighbor.visited: search(neighbor)","title":"Depth-First Search (DFS)"},{"location":"graphs/#breadth-first-search-bfs","text":"BFS is a bit less intuitive, and many interviewees struggle with the implementation unless they are already familiar with it. The main tripping point is the (false) assumption that BFS is recursive. It's not. Instead, it uses a queue. In BFS, node a visits each of a 's neighbors before visiting any of their neighbors. You can think of this as searching level by level out from a . An iterative solution involving a queue usually works best. The time complexity can be expressed as O(|V| + |E|) , since every vertex and every edge will be explored in the worst case. |V| is the number of vertices and |E| is the number of edges in the graph. Note that O(|E|) may vary between O(1) and O(|V|^2) , depending on how sparse the input graph is. def search(root): queue = [] root.marked = True queue.append(root) while len(queue) 0: node = queue.pop(0) visit(node) for neighbor in node.neighbors: if not neighbor.visited: queue.append(neighbor) If you are asked to implement BFS, the key thing to remember is the use of the queue. The rest of the algo\u00adrithm flows from this fact.","title":"Breadth-First Search (BFS)"},{"location":"graphs/#bidirectional-search","text":"Bidirectional search is used to find the shortest path between a source and destination node. It operates by essentially running two simultaneous breadth-first searches, one from each node. When their searches collide, we have found a path. To see why this is faster, consider a graph where every node has at most k adjacent nodes and the shortest path from node s to nodet has length d. In traditional breadth-first search, we would search up to k nodes in the first \"level\" of the search. In the second level, we would search up to k nodes for each of those first k nodes, so k2 nodes total (thus far). We would do this d times, so that's 0( kd) nodes. In bidirectional search, we have two searches that collide after approximately \ufffd levels (the midpoint of the path). The search from s visits approximately kd12, as does the search fromt.That's approximately 2 kdl2, or 0( kd/2), nodes total. This might seem like a minor difference, but it's not. It's huge. Recall that tional search is actually faster by a factor of kd12. Put another way: if our system could only support searching \"friend of friend\" paths in breadth-first search, it could now likely support \"friend of friend of friend of friend\" paths. We can support paths that are twice as long.","title":"Bidirectional Search"},{"location":"graphs/#topological-sorting","text":"In computer science, a topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v , u comes before v in the ordering. For instance, the vertices of the graph may represent tasks to be performed, and the edges may represent constraints that one task must be performed before another; in this application, a topological ordering is just a valid sequence for the tasks. A topological ordering is possible if and only if the graph has no directed cycles, that is, if it is a directed acyclic graph (DAG). Any DAG has at least one topological ordering, and algorithms are known for constructing a topological ordering of any DAG in linear time. YouTube DFS An alternative algorithm for topological sorting is based on depth-first search. The algorithm loops through each node of the graph, in an arbitrary order, initiating a depth-first search that terminates when it hits any node that has already been visited since the beginning of the topological sort or the node has no outgoing edges (i.e. a leaf node). Since each edge and node is visited once, the algorithm runs in linear time. L \u2190 Empty list that will contain the sorted nodes while exists nodes without a permanent mark do select an unmarked node n visit(n) function visit(node n) if n has a permanent mark then return if n has a temporary mark then stop (not a DAG) mark n with a temporary mark for each node m with an edge from n to m do visit(m) remove temporary mark from n mark n with a permanent mark add n to head of L","title":"Topological Sorting"},{"location":"heaps/","text":"Heap Problems Binary Heaps (Min-Heaps and Max-Heaps) A min-heap is a complete binary tree (that is, totally filled other than the rightmost elements on the last level) where each node is smaller than its children. The root, therefore, is the minimum element in the tree. Insert When we insert into a min-heap, we always start by inserting the element at the bottom. We insert at the rightmost spot so as to maintain the complete tree property. Then, we \"fix\" the tree by swapping the new element with its parent, until we find an appropriate spot for the element. We essentially bubble up the minimum element. This takes O(log n) time, where n is the number of nodes in the heap. Extract Minimum Element Finding the minimum element of a min-heap is easy: it's always at the top. The trickier part is how to remove it. (In fact, this isn't that tricky.) First, we remove the minimum element and swap it with the last element in the heap (the bottommost, rightmost element). Then, we bubble down this element, swapping it with one of its children until the min\u00ad heap property is restored. Do we swap it with the left child or the right child? That depends on their values. There's no inherent ordering between the left and right element, but you'll need to take the smaller one in order to maintain the min-heap ordering. This algorithm will also take O(log n) time. Implementation Usage Note, the default heap implementation is a min-heap. To use a max-heap, invert the value of the keys. Heap elements can also be tuples. import heapq numbers = [5, 7, 9, 1, 3] # Use heapify to convert a list into a heap. heapq.heapify(numbers) # Use heappush() to push elements. heapq.heappush(numbers, 4) # Use heappop() to pop smallest element. print(heapq.heappop(numbers))","title":"Heaps"},{"location":"heaps/#implementation","text":"","title":"Implementation"},{"location":"heaps/#usage","text":"Note, the default heap implementation is a min-heap. To use a max-heap, invert the value of the keys. Heap elements can also be tuples. import heapq numbers = [5, 7, 9, 1, 3] # Use heapify to convert a list into a heap. heapq.heapify(numbers) # Use heappush() to push elements. heapq.heappush(numbers, 4) # Use heappop() to pop smallest element. print(heapq.heappop(numbers))","title":"Usage"},{"location":"linked-lists/","text":"Linked List Problems Two Pointers Problems A linked list is a data structure that represents a sequence of nodes. In a singly linked list, each node points to the next node in the linked list. A doubly linked list gives each node pointers to both the next node and the previous node. Unlike an array, a linked list does not provide constant time access to a particular \"index\" within the list. This means that if you'd like to find the Kth element in the list, you will need to iterate through K elements. The benefit of a linked list is that you can add and remove items from the beginning of the list in constant time. For specific applications, this can be useful. A number of linked list problems rely on recursion. If you're having trouble solving a linked list problem, you should explore if a recursive approach will work. Remember that recursive algorithms take at least O(n) space, where n is the depth of the recursive call. All recursive algorithms can be implemented iteratively, although they may be much more complex. Implementation class SingleNode: def __init__(self, value=None): self.value = value self.next = None class SingleLinkedList: def __init__(self): self.head = None def print(self): node = self.head while node is not None: print(node.value) node = node.next def append_to_tail(self, value): end = Node(value) node = self.head while node.next is not None: node = node.next node.next = end def delete_node(self, value): node = self.head if node.value == value: self.head = node.next while node.next is not None: if node.next.value == value: node.next = node.next.next node = node.next class DoubleNode: def __init__(self, value=None): self.value = value self.next = None self.prev = None class DoubleLinkedList: def __init__(self): self.head = None def print(self): node = self.head while node is not None: print(node.value) node = node.next The \"Runner\" Technique The \"runner\" (or second pointer) technique is used in many linked list problems. The runner technique means that you iterate through the linked list with two pointers simultaneously, with one ahead of the other. The \"fast\" node might be ahead by a fixed amount, or it might be hopping multiple nodes for each one node that the \"slow\" node iterates through.","title":"Linked Lists"},{"location":"linked-lists/#implementation","text":"class SingleNode: def __init__(self, value=None): self.value = value self.next = None class SingleLinkedList: def __init__(self): self.head = None def print(self): node = self.head while node is not None: print(node.value) node = node.next def append_to_tail(self, value): end = Node(value) node = self.head while node.next is not None: node = node.next node.next = end def delete_node(self, value): node = self.head if node.value == value: self.head = node.next while node.next is not None: if node.next.value == value: node.next = node.next.next node = node.next class DoubleNode: def __init__(self, value=None): self.value = value self.next = None self.prev = None class DoubleLinkedList: def __init__(self): self.head = None def print(self): node = self.head while node is not None: print(node.value) node = node.next","title":"Implementation"},{"location":"linked-lists/#the-runner-technique","text":"The \"runner\" (or second pointer) technique is used in many linked list problems. The runner technique means that you iterate through the linked list with two pointers simultaneously, with one ahead of the other. The \"fast\" node might be ahead by a fixed amount, or it might be hopping multiple nodes for each one node that the \"slow\" node iterates through.","title":"The \"Runner\" Technique"},{"location":"math-and-logic-puzzles/","text":"","title":"Math and logic puzzles"},{"location":"miscellaneous/","text":"Grids and Matricies Is BFS/DFS applicable? Can we use Dynamic Programming for cells based on surrounding cells?","title":"Miscellaneous"},{"location":"miscellaneous/#grids-and-matricies","text":"Is BFS/DFS applicable? Can we use Dynamic Programming for cells based on surrounding cells?","title":"Grids and Matricies"},{"location":"ood/","text":"How to Approach 1. Handle Ambiguity Ask clarifying questions. When being asked about an object-oreinted design question, you should inquire who is going to use it and how they are going to use it. 2. Define the Core Objects 3. Analyze Relationships Which objects are members of which ohter objects? Do any objects inherit from any others? Are relationships many-to-many or one-to-many? 4. Investigate Actions Design Patterns Be careful you don't fall into a trap of constantly trying to find the \"right\" design pattern for a particular problem. You should create the design that works for that problem. In some cases it might be an established pattern, but in many other cases it is not. Singleton Class The Singleton pattern ensures that a class has only one instance and ensures access to the instance through the application. It can be useful in cases where you have a \"global\" object with exactly one instance. For example, we may want to implement Restaurant such that it has exactly one instance of Restaurant . public class Restaurant { private static Restaurant _instance = null; protected Retaurant() { ... } public static Restaurant getInstance() { if (_instance == null) { _instance = new Restaurant(); } } } It should be noted that many people dislike the Singleton design pattern, even calling it an \"anti-pattern:' One reason for this is that it can interfere with unit testing. Factory Method The Factory Method offers an interface for creating an instance of a class, with its subclasses deciding which class to instantiate. You might want to implement this with the creator class being abstract and not providing an implementation for the Factory method. Or, you could have the Creator class be a concrete class that provides an implementation for the Factory method. In this case, the Factory method would take a parameter representing which class to instantiate. public class CardGame { public static CardGame createCardGame(GameType type) { if (type == GameType.Poker) { return new PokerGame(); } else if (type == GameType.BlackJack) { return new BlackJackGame(); } return null; } }","title":"Object-Oriented Design"},{"location":"ood/#how-to-approach","text":"1. Handle Ambiguity Ask clarifying questions. When being asked about an object-oreinted design question, you should inquire who is going to use it and how they are going to use it. 2. Define the Core Objects 3. Analyze Relationships Which objects are members of which ohter objects? Do any objects inherit from any others? Are relationships many-to-many or one-to-many? 4. Investigate Actions","title":"How to Approach"},{"location":"ood/#design-patterns","text":"Be careful you don't fall into a trap of constantly trying to find the \"right\" design pattern for a particular problem. You should create the design that works for that problem. In some cases it might be an established pattern, but in many other cases it is not. Singleton Class The Singleton pattern ensures that a class has only one instance and ensures access to the instance through the application. It can be useful in cases where you have a \"global\" object with exactly one instance. For example, we may want to implement Restaurant such that it has exactly one instance of Restaurant . public class Restaurant { private static Restaurant _instance = null; protected Retaurant() { ... } public static Restaurant getInstance() { if (_instance == null) { _instance = new Restaurant(); } } } It should be noted that many people dislike the Singleton design pattern, even calling it an \"anti-pattern:' One reason for this is that it can interfere with unit testing. Factory Method The Factory Method offers an interface for creating an instance of a class, with its subclasses deciding which class to instantiate. You might want to implement this with the creator class being abstract and not providing an implementation for the Factory method. Or, you could have the Creator class be a concrete class that provides an implementation for the Factory method. In this case, the Factory method would take a parameter representing which class to instantiate. public class CardGame { public static CardGame createCardGame(GameType type) { if (type == GameType.Poker) { return new PokerGame(); } else if (type == GameType.BlackJack) { return new BlackJackGame(); } return null; } }","title":"Design Patterns"},{"location":"python-tricks/","text":"You can return a dicitonary\u2019s values! Tuple sorting uses all values by default! Handy float('-inf') String, remove at index, use del Enumerate s","title":"Python tricks"},{"location":"queues/","text":"Queue Problems A queue implements FIFO (first-in first-out) ordering. As in a line or queue at a ticket stand, items are removed from the data structure in the same order that they are added. It uses the operations: add(item) : Add an item to the end of the list. remove() : Remove the first item in the list. peek() : Return the top of the queue. is_empty() : Return True if and only if the queue is empty. A queue can also be implemented with a linked list. In fact, they are essentially the same thing, as long as items are added and removed from opposite sides. Implementation class Queue: def __init__(self): self.queue = [] def add(self, item): self.queue.append(item) def remove(self): if len(self.queue) 0: return self.queue.pop(0) return None def peek(self): if len(self.queue) 0: return self.queue[0] return None def is_empty(self): return len(self.queue) == 0 It is especially easy to mess up the updating of the first and last nodes in a queue. Be sure to double check this. One place where queues are often used is in breadth-first search or in implementing a cache. In breadth-first search, for example, we used a queue to store a list of the nodes that we need to process. Each time we process a node, we add its adjacent nodes to the back of the queue. This allows us to process nodes in the order in which they are viewed. Usage List fruits = [] # Add to queue. fruits.append('banana') fruits.append('grapes') # Remove from queue. first_item = fruits.pop(0) deque from collections import deque numbers = deque() numbers.append(99) numbers.append(15) last_item = numbers.popleft()","title":"Queues"},{"location":"queues/#implementation","text":"class Queue: def __init__(self): self.queue = [] def add(self, item): self.queue.append(item) def remove(self): if len(self.queue) 0: return self.queue.pop(0) return None def peek(self): if len(self.queue) 0: return self.queue[0] return None def is_empty(self): return len(self.queue) == 0 It is especially easy to mess up the updating of the first and last nodes in a queue. Be sure to double check this. One place where queues are often used is in breadth-first search or in implementing a cache. In breadth-first search, for example, we used a queue to store a list of the nodes that we need to process. Each time we process a node, we add its adjacent nodes to the back of the queue. This allows us to process nodes in the order in which they are viewed.","title":"Implementation"},{"location":"queues/#usage","text":"List fruits = [] # Add to queue. fruits.append('banana') fruits.append('grapes') # Remove from queue. first_item = fruits.pop(0) deque from collections import deque numbers = deque() numbers.append(99) numbers.append(15) last_item = numbers.popleft()","title":"Usage"},{"location":"recursion-and-dynamic-programming/","text":"Recursion Problems Divide and Conquer Problems Dynamic Programming Problems Memoization Problems While there are a large number of recursive problems, many follow similar patterns. A good hint that a problem is recursive is that it can be built off of subproblems. How to Approach Bottom-Up Approach Top-Down Approach Half-and-Half Approach Drawing the recursive calls as a tree is a great way to figure out the runtime of a recursive algorithm. Divide-and-conquer algorithm Recursive vs. Iterative Solutions Recursive algorithms can be very space inefficient. Each recursive call adds a new layer to the stack, which means that if your algorithm recurses to a depth of n , it uses at least O(n) memory. For this reason, it's often better to implement a recursive algorithm iteratively. All recursive algorithms can be implemented iteratively, although sometimes the code to do so is much more complex. Before diving into recursive code, ask yourself how hard it would be to implement it iteratively, and discuss the tradeoffs with your interviewer. Dynamic Programming Memoization Dynamic programming is mostly just a matter of taking a recursive algorithm and finding the overlapping subproblems (that is, the repeated calls). You then cache those results for future recursive calls. Generate Fibonacci Numbers Recursive def fibonacci(n) { if n == 0 or n == 1: return n return fibonacci(n - 1) + fibonacci(n - 2) } Top-Down Dynamic Programming (or Memoization ) int fibonacci(int n) { return fibonacci(n, new int[n + 1]); } int fibonacci(int i, int[] memo) { if(i== 0 || i== 1) return i; if (memo[i] == 0) { memo[i] = fibonacci(i - 1, memo) + fibonacci(i - 2, memo); } return memo[i] } Bottom-Up Dynamic Programming TODO TODO","title":"Recursion & Dynamic Programming"},{"location":"recursion-and-dynamic-programming/#recursive-vs-iterative-solutions","text":"Recursive algorithms can be very space inefficient. Each recursive call adds a new layer to the stack, which means that if your algorithm recurses to a depth of n , it uses at least O(n) memory. For this reason, it's often better to implement a recursive algorithm iteratively. All recursive algorithms can be implemented iteratively, although sometimes the code to do so is much more complex. Before diving into recursive code, ask yourself how hard it would be to implement it iteratively, and discuss the tradeoffs with your interviewer.","title":"Recursive vs. Iterative Solutions"},{"location":"recursion-and-dynamic-programming/#dynamic-programming-memoization","text":"Dynamic programming is mostly just a matter of taking a recursive algorithm and finding the overlapping subproblems (that is, the repeated calls). You then cache those results for future recursive calls. Generate Fibonacci Numbers Recursive def fibonacci(n) { if n == 0 or n == 1: return n return fibonacci(n - 1) + fibonacci(n - 2) } Top-Down Dynamic Programming (or Memoization ) int fibonacci(int n) { return fibonacci(n, new int[n + 1]); } int fibonacci(int i, int[] memo) { if(i== 0 || i== 1) return i; if (memo[i] == 0) { memo[i] = fibonacci(i - 1, memo) + fibonacci(i - 2, memo); } return memo[i] } Bottom-Up Dynamic Programming TODO TODO","title":"Dynamic Programming &amp; Memoization"},{"location":"sorting-and-searching/","text":"Sorting Algorithms Of the five algorithms explained below, Merge Sort, Quick Sort and Bucket Sort are the most commonly used in interviews. Bubble Sort | Runtime: O(n^2) average and worst case. Memory: O(1) . In bubble sort, we start at the beginning of the array and swap the first two elements if the first is greater than the second. Then, we go to the next pair, and so on, continuously making sweeps of the array until it is sorted. In doing so, the smaller items slowly \"bubble\" up to the beginning of the list. Selection Sort | Runtime: O(n^2) average and worst case. Memory: O(1) . Selection sort is the child's algorithm: simple, but inefficient. Find the smallest element using a linear scan and move it to the front (swapping it with the front element). Then, find the second smallest and move it, again doing a linear scan. Continue doing this until all the elements are in place. Merge Sort | Runtime: O(n log(n)) average and worst case. Memory: Depends . Merge sort divides the array in half, sorts each of those halves, and then merges them back together. Each of those halves has the same sorting algorithm applied to it. Eventually, you are merging just two single\u00ad element arrays. It is the \"merge\" part that does all the heavy lifting. TODO The space complexity of merge sort is O(n) due to the auxiliary space used to merge parts of the array. Quick Sort | Runtime: O(n log(n)) average, O(n^2) worst case. Memory: O(log(n)) . In quick sort we pick a random element and partition the array, such that all numbers that are less than the partitioning element come before all elements that are greater than it. The partitioning can be performed efficiently through a series of swaps (see below). If we repeatedly partition the array (and its sub-arrays) around an element, the array will eventually become sorted. However, as the partitioned element is not guaranteed to be the median (or anywhere near the median), our sorting could be very slow. This is the reason for the O(n^2) worst case runtime. TODO Radix Sort | Runtime: O(kn) Radix sort is a sorting algorithm for integers (and some other data types) that takes advantage of the fact that integers have a finite number of bits. In radix sort, we iterate through each digit of the number, grouping numbers by each digit. For example, if we have an array of integers, we might first sort by the first digit, so that the Os are grouped together. Then, we sort each of these groupings by the next digit. We repeat this process sorting by each subsequent digit, until finally the whole array is sorted. Unlike comparison sorting algorithms, which cannot perform better than O(n log(n)) in the average case, radix sort has a runtime of O(kn) , where n is the number of elements and k is the number of passes of the sorting algorithm. Python Sorting sorted(list, key=lambda item: get_key_for_item(item), reverse=True) # reverse is False by default Searching Algorithms In binary search, we look for an element x in a sorted array by first comparing x to the midpoint of the array. If x is less than the midpoint, then we search the left half of the array. If x is greater than the midpoint, then we search the right half of the array. We then repeat this process, treating the left and right halves as subar\u00adrays. Again, we compare x to the midpoint of this subarray and then search either its left or right side. We repeat this process until we either find x or the subarray has size 0 . Note that although the concept is fairly simple, getting all the details right is far more difficult than you might think. As you study the code below, pay attention to the plus ones and minus ones. def binary_search(array, target): left, right = 0, len(array) - 1 while left = right: middle = (left + right) // 2 if array[middle] == target: return middle if target array[middle]: right = middle - 1 else: left = middle + 1 return -1 Potential ways to search a data structure extend beyond binary search, and you would do best not to limit yourself to just this option. You might, for example, search for a node by leveraging a binary tree, or by using a hash table. Think beyond binary search!","title":"Sorting & Searching"},{"location":"sorting-and-searching/#sorting-algorithms","text":"Of the five algorithms explained below, Merge Sort, Quick Sort and Bucket Sort are the most commonly used in interviews. Bubble Sort | Runtime: O(n^2) average and worst case. Memory: O(1) . In bubble sort, we start at the beginning of the array and swap the first two elements if the first is greater than the second. Then, we go to the next pair, and so on, continuously making sweeps of the array until it is sorted. In doing so, the smaller items slowly \"bubble\" up to the beginning of the list. Selection Sort | Runtime: O(n^2) average and worst case. Memory: O(1) . Selection sort is the child's algorithm: simple, but inefficient. Find the smallest element using a linear scan and move it to the front (swapping it with the front element). Then, find the second smallest and move it, again doing a linear scan. Continue doing this until all the elements are in place. Merge Sort | Runtime: O(n log(n)) average and worst case. Memory: Depends . Merge sort divides the array in half, sorts each of those halves, and then merges them back together. Each of those halves has the same sorting algorithm applied to it. Eventually, you are merging just two single\u00ad element arrays. It is the \"merge\" part that does all the heavy lifting. TODO The space complexity of merge sort is O(n) due to the auxiliary space used to merge parts of the array. Quick Sort | Runtime: O(n log(n)) average, O(n^2) worst case. Memory: O(log(n)) . In quick sort we pick a random element and partition the array, such that all numbers that are less than the partitioning element come before all elements that are greater than it. The partitioning can be performed efficiently through a series of swaps (see below). If we repeatedly partition the array (and its sub-arrays) around an element, the array will eventually become sorted. However, as the partitioned element is not guaranteed to be the median (or anywhere near the median), our sorting could be very slow. This is the reason for the O(n^2) worst case runtime. TODO Radix Sort | Runtime: O(kn) Radix sort is a sorting algorithm for integers (and some other data types) that takes advantage of the fact that integers have a finite number of bits. In radix sort, we iterate through each digit of the number, grouping numbers by each digit. For example, if we have an array of integers, we might first sort by the first digit, so that the Os are grouped together. Then, we sort each of these groupings by the next digit. We repeat this process sorting by each subsequent digit, until finally the whole array is sorted. Unlike comparison sorting algorithms, which cannot perform better than O(n log(n)) in the average case, radix sort has a runtime of O(kn) , where n is the number of elements and k is the number of passes of the sorting algorithm. Python Sorting sorted(list, key=lambda item: get_key_for_item(item), reverse=True) # reverse is False by default","title":"Sorting Algorithms"},{"location":"sorting-and-searching/#searching-algorithms","text":"In binary search, we look for an element x in a sorted array by first comparing x to the midpoint of the array. If x is less than the midpoint, then we search the left half of the array. If x is greater than the midpoint, then we search the right half of the array. We then repeat this process, treating the left and right halves as subar\u00adrays. Again, we compare x to the midpoint of this subarray and then search either its left or right side. We repeat this process until we either find x or the subarray has size 0 . Note that although the concept is fairly simple, getting all the details right is far more difficult than you might think. As you study the code below, pay attention to the plus ones and minus ones. def binary_search(array, target): left, right = 0, len(array) - 1 while left = right: middle = (left + right) // 2 if array[middle] == target: return middle if target array[middle]: right = middle - 1 else: left = middle + 1 return -1 Potential ways to search a data structure extend beyond binary search, and you would do best not to limit yourself to just this option. You might, for example, search for a node by leveraging a binary tree, or by using a hash table. Think beyond binary search!","title":"Searching Algorithms"},{"location":"stacks/","text":"Stack Problems The stack data structure is precisely what it sounds like: a stack of data. In certain types of problems, it can be favorable to store data in a stack rather than in an array. A stack uses LIFO (last-in first-out) ordering. That is, as in a stack of dinner plates, the most recent item added to the stack is the first item to be removed. It uses the following operations: pop() : Remove the top item from the stack. push(item) : Add an item to the top of the stack. peek() : Return the top of the stack. is_empty() : Return True if and only if the stack is empty. Implementation class Stack: def __init__(self): self.stack = [] def pop(self): if len(self.stack) 0: return self.stack.pop() return None def push(self, item): self.stack.append(item) def peek(self): if len(self.stack) 0: return self.stack[len(self.stack) - 1] return None def is_empty(self): return len(self.stack) == 0 One case where stacks are often useful is in certain recursive algorithms. Sometimes you need to push temporary data onto a stack as you recurse, but then remove them as you backtrack (for example, because the recursive check failed). A stack offers an intuitive way to do this. A stack can also be used to implement a recursive algorithm iteratively. Usage List letters = [] # Add to stack. letters.append('a') letters.append('b') # Remove from stack. last_item = letters.pop() deque from collections import deque numbers = deque() # Use append like before to add elements. numbers.append(99) numbers.append(15) # You can pop like a stack. last_item = numbers.pop()","title":"Stacks"},{"location":"stacks/#implementation","text":"class Stack: def __init__(self): self.stack = [] def pop(self): if len(self.stack) 0: return self.stack.pop() return None def push(self, item): self.stack.append(item) def peek(self): if len(self.stack) 0: return self.stack[len(self.stack) - 1] return None def is_empty(self): return len(self.stack) == 0 One case where stacks are often useful is in certain recursive algorithms. Sometimes you need to push temporary data onto a stack as you recurse, but then remove them as you backtrack (for example, because the recursive check failed). A stack offers an intuitive way to do this. A stack can also be used to implement a recursive algorithm iteratively.","title":"Implementation"},{"location":"stacks/#usage","text":"List letters = [] # Add to stack. letters.append('a') letters.append('b') # Remove from stack. last_item = letters.pop() deque from collections import deque numbers = deque() # Use append like before to add elements. numbers.append(99) numbers.append(15) # You can pop like a stack. last_item = numbers.pop()","title":"Usage"},{"location":"trees/","text":"Tree Problems Binary Search Tree Problems Types of Trees A nice way to understand a tree is with a recursive explanation. A tree is a data structure composed of nodes. Each tree has a root node. (Actually, this isn't strictly necessary in graph theory, but it's usually how we use trees in programming, and especially programming interviews.) The root node has zero or more child nodes. Each child node has zero or more child nodes, and so on. The tree cannot contain cycles. The nodes may or may not be in a particular order, they could have any data type as values, and they may or may not have links back to their parent nodes. Implementation class TreeNode: def __init__(self, value=None, children=[]): self.value = value self.children = children class BinaryTreeNode: def __init__(self, value=0, left=None, right=None): self.value = value self.left = left self.right = right Trees vs. Binary Trees A binary tree is a tree in which each node has up to two children. Not all trees are binary trees. There are occasions when you might have a tree that is not a binary tree. For example, suppose you were using a tree to represent a bunch of phone numbers. In this case, you might use a 10-ary tree, with each node having up to 10 children (one for each digit). A node is called a \"leaf\" node if it has no children. Binary Tree vs. Binary Search Tree A binary search tree is a binary tree in which every node fits a specific ordering property: all left descendents = n all right descendents .This must be true for each node n . The definition of a binary search tree can vary slightly with respect to equality. Under some defi\u00adnitions, the tree cannot have duplicate values. In others, the duplicate values will be on the right or can be on either side. All are valid definitions, but you should clarify this with your interviewer. When given a tree question, many candidates assume the interviewer means a binary search tree. Be sure to ask. A binary search tree imposes the condition that, for each node, its left descendents are less than or equal to the current node, which is less than the right descendents. Balanced vs. Unbalanced While many trees are balanced, not all are. Ask your interviewer for clarification here. Note that balancing a tree does not mean the left and right subtrees are exactly the same size. Complete Binary Trees A complete binary tree is a binary tree in which every level of the tree is fully filled, except for perhaps the last level. To the extent that the last level is filled, it is filled left to right. Full Binary Trees A full binary tree is a binary tree in which every node has either zero or two children. That is, no nodes have only one child. Perfect Binary Trees A perfect binary tree is one that is both full and complete. All leaf nodes will be at the same level, and this level has the maximum number of nodes. Note that perfect trees are rare in interviews and in real life, as a perfect tree must have exactly 2^k - 1 nodes (where k is the number of levels). In an interview, do not assume a binary tree is perfect. Traversal Prior to your interview, you should be comfortable implementing in-order, post-order, and pre-order traversal. The most common of these is in-order traversal. In-Order Traversal In-order traversal means to \"visit\" (often, print) the left branch, then the current node, and finally, the right branch. def recursive_in_order_traversal(node): if node != None: in_order_traversal(node.left) print(node.value) in_order_traversal(node.right) def iterative_in_order_traversal(root): if root is None: return current = root stack = [] while True: if current is not None: stack.append(current) current = current.left elif len(stack) 0: current = stack.pop() print(current.value) current = current.right else: break When performed on a binary search tree, it visits the nodes in ascending order (hence the name \"in-order\"). Pre-Order Traversal Pre-order traversal visits the current node before its child nodes (hence the name \"pre-order\"). def recursive_pre_order_traversal(node): if node != None: print(node.value) pre_order_traversal(node.left) pre_order_traversal(node.right) def iterative_pre_order_traversal(root): if root is None: return stack = [] stack.append(root) while len(stack) 0: node = stack.pop() print(node.value) if node.right is not None: stack.append(node.right) if node.left is not None: stack.append(node.left) In a pre-order traversal, the root is always the first node visited. Post-Order Traversal Post-order traversal visits the current node after its child nodes (hence the name\"post-order\"). def post_order_traversal(node): if node != None: post_order_traversal(node.left) post_order_traversal(node.right) print(node.value) # TODO: Review and test this. def iterative_post_order_traversal(root): if root is None: return stack = [] while(True): while (root): if root.right is not None: stack.append(root.right) stack.append(root) root = root.left root = stack.pop() if (root.right is not None and peek(stack) == root.right): stack.pop() stack.append(root) root = root.right else: ans.append(root.value) root = None if len(stack) = 0: break In a post-order traversal, the root is always the last node visited.","title":"Trees"},{"location":"trees/#implementation","text":"class TreeNode: def __init__(self, value=None, children=[]): self.value = value self.children = children class BinaryTreeNode: def __init__(self, value=0, left=None, right=None): self.value = value self.left = left self.right = right Trees vs. Binary Trees A binary tree is a tree in which each node has up to two children. Not all trees are binary trees. There are occasions when you might have a tree that is not a binary tree. For example, suppose you were using a tree to represent a bunch of phone numbers. In this case, you might use a 10-ary tree, with each node having up to 10 children (one for each digit). A node is called a \"leaf\" node if it has no children. Binary Tree vs. Binary Search Tree A binary search tree is a binary tree in which every node fits a specific ordering property: all left descendents = n all right descendents .This must be true for each node n . The definition of a binary search tree can vary slightly with respect to equality. Under some defi\u00adnitions, the tree cannot have duplicate values. In others, the duplicate values will be on the right or can be on either side. All are valid definitions, but you should clarify this with your interviewer. When given a tree question, many candidates assume the interviewer means a binary search tree. Be sure to ask. A binary search tree imposes the condition that, for each node, its left descendents are less than or equal to the current node, which is less than the right descendents. Balanced vs. Unbalanced While many trees are balanced, not all are. Ask your interviewer for clarification here. Note that balancing a tree does not mean the left and right subtrees are exactly the same size. Complete Binary Trees A complete binary tree is a binary tree in which every level of the tree is fully filled, except for perhaps the last level. To the extent that the last level is filled, it is filled left to right. Full Binary Trees A full binary tree is a binary tree in which every node has either zero or two children. That is, no nodes have only one child. Perfect Binary Trees A perfect binary tree is one that is both full and complete. All leaf nodes will be at the same level, and this level has the maximum number of nodes. Note that perfect trees are rare in interviews and in real life, as a perfect tree must have exactly 2^k - 1 nodes (where k is the number of levels). In an interview, do not assume a binary tree is perfect.","title":"Implementation"},{"location":"trees/#traversal","text":"Prior to your interview, you should be comfortable implementing in-order, post-order, and pre-order traversal. The most common of these is in-order traversal. In-Order Traversal In-order traversal means to \"visit\" (often, print) the left branch, then the current node, and finally, the right branch. def recursive_in_order_traversal(node): if node != None: in_order_traversal(node.left) print(node.value) in_order_traversal(node.right) def iterative_in_order_traversal(root): if root is None: return current = root stack = [] while True: if current is not None: stack.append(current) current = current.left elif len(stack) 0: current = stack.pop() print(current.value) current = current.right else: break When performed on a binary search tree, it visits the nodes in ascending order (hence the name \"in-order\"). Pre-Order Traversal Pre-order traversal visits the current node before its child nodes (hence the name \"pre-order\"). def recursive_pre_order_traversal(node): if node != None: print(node.value) pre_order_traversal(node.left) pre_order_traversal(node.right) def iterative_pre_order_traversal(root): if root is None: return stack = [] stack.append(root) while len(stack) 0: node = stack.pop() print(node.value) if node.right is not None: stack.append(node.right) if node.left is not None: stack.append(node.left) In a pre-order traversal, the root is always the first node visited. Post-Order Traversal Post-order traversal visits the current node after its child nodes (hence the name\"post-order\"). def post_order_traversal(node): if node != None: post_order_traversal(node.left) post_order_traversal(node.right) print(node.value) # TODO: Review and test this. def iterative_post_order_traversal(root): if root is None: return stack = [] while(True): while (root): if root.right is not None: stack.append(root.right) stack.append(root) root = root.left root = stack.pop() if (root.right is not None and peek(stack) == root.right): stack.pop() stack.append(root) root = root.right else: ans.append(root.value) root = None if len(stack) = 0: break In a post-order traversal, the root is always the last node visited.","title":"Traversal"},{"location":"tries/","text":"Trie Problems A trie is a variant of an n-ary tree in which characters are stored at each node. Each path down the tree may repr The * nodes (sometimes called \"null nodes\") are often used to indicate complete words. The actual implementation of these * nodes might be a special type of child (such as a TerminatingTrieNode , which inherits from TrieNode ). Or, we could use just a boolean flag terminates within the \"parent\" node. A node in a trie could have anywhere from 1 through ALPHABET_SIZE + 1 children (or, 0 through ALPHABET_SIZE if a boolean flag is used instead of a * node). Very commonly, a trie is used to store the entire (English) language for quick prefix lookups. While a hash table can quickly look up whether a string is a valid word, it cannot tell us if a string is a prefix of any valid words. A trie can do this very quickly. How quickly? A trie can check if a string is a valid prefix in O(K) time, where K is the length of the string. This is actually the same runtime as a hash table will take. Although we often refer to hash table lookups as being O(1) time, this isn't entirely true. A hash table must read through all the characters in the input, which takes O(K) time in the case of a word lookup. Implementation # TODO: ... Usage # TODO: ...","title":"Tries (Prefix Trees)"},{"location":"tries/#implementation","text":"# TODO: ...","title":"Implementation"},{"location":"tries/#usage","text":"# TODO: ...","title":"Usage"}]}